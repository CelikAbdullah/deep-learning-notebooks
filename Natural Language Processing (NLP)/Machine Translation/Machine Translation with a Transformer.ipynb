{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CelikAbdullah/deep-learning-notebooks/blob/main/Natural%20Language%20Processing%20(NLP)/Machine%20Translation/Machine%20Translation%20with%20a%20Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "O77KC4MyIh1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2MXpIL6FwmE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to implement a sequence-to-sequence modeling on a machine translation task."
      ],
      "metadata": {
        "id": "tocgc9OUIm1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset"
      ],
      "metadata": {
        "id": "S5CtHu8vIp2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have to download an English-to-Spanish translation dataset from the following download link:"
      ],
      "metadata": {
        "id": "wd9thvNQIv5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA8NcxMyIwqP",
        "outputId": "e37ee751-1d20-4031-8c67-547d9b0a451e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-15 15:59:03--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.219.207, 209.85.146.207, 209.85.147.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.219.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-09-15 15:59:03 (240 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete the download, we unzip the .zip file:"
      ],
      "metadata": {
        "id": "NEMQA174Iy3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "id": "sRnzqdZjI3yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse the text file"
      ],
      "metadata": {
        "id": "GIYOAQ0qI5wN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text file contains one example per line:\n",
        "\n",
        "**[an English sentence] [tab character] [corresponding Spanish sentence]**\n",
        "\n",
        "Let's parse the .txt file:"
      ],
      "metadata": {
        "id": "2YUeDPBtI9_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "\n",
        "# iterate over the lines in the file\n",
        "for line in lines:\n",
        "  # each line contains an English phrase and its Spanish translation\n",
        "  # a Tab separates them\n",
        "  english, spanish = line.split(\"\\t\")\n",
        "  # prepend [start] and append [end] to the Spanish sentence\n",
        "  spanish = \"[start] \" + spanish + \" [end]\"\n",
        "  text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "fjiEzJ3LI7f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print a random sentence to see how it looks like:"
      ],
      "metadata": {
        "id": "Y3dD5bcGJEwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_example = random.choice(text_pairs)\n",
        "print(random_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHnMnqS4JFhp",
        "outputId": "e2dbffb7-66a4-462a-9950-3064257088de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Perhaps you have misunderstood the aim of our project.', '[start] Puede que hayas entendido mal el objetivo de nuestro proyecto. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the dataset"
      ],
      "metadata": {
        "id": "RlTpZyngJILR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's shuffle the dataset and split it into a training, validation and test sets:"
      ],
      "metadata": {
        "id": "59_R8r0mJOdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle\n",
        "random.shuffle(text_pairs)\n",
        "# calculate number of validation samples\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "# calculate number of training samples\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "# training set\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "# validation set\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "# test set\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "UYh7iUm-JKiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize the English and Spanish text pairs"
      ],
      "metadata": {
        "id": "XV4Tkl0HJTHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create two TextVectorization layers: one for English and one for Spanish.\n",
        "\n",
        "For that, we preserve the [start] and [end] tokens that we inserted previously. Keep in mind that punctuation is different in each language. In the Spanish TextVectorization layer, if we are going to strip punctuation characters, we need to also strip the character \"¿\". Normally, we wouldn't do that but for the sake of simplicity, we'll do it here."
      ],
      "metadata": {
        "id": "wh16zTXPJUHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "# a custom string standardization function\n",
        "# for the Spanish TextVectorization layer:\n",
        "# it preserves [ and ] but strips ¿ as well\n",
        "# as other characters from strings.punctuation\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")"
      ],
      "metadata": {
        "id": "hRJaoZ5VJXD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to keep things simple, we'll only look at the\n",
        "# top 15000 words in each language\n",
        "vocab_size = 15000\n",
        "# we'll also restrict sentences to 20 words\n",
        "sequence_length = 20\n",
        "\n",
        "# define the English TextVectorization layer\n",
        "source_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "# define the Spanish TextVectorization layer\n",
        "target_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    # recall: that each Spanish sentence starts with the \"[start]\" token\n",
        "    #         so, we need to offset the sentence by one step during training\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "# invoke adapt to learn the vocabulary of each language\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "m3IXuIx9JY22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turn data into a tf.data pipeline"
      ],
      "metadata": {
        "id": "OVdD6if0JaQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to return a tuple (inputs, target) where inputs is a dict with two key, “encoder_inputs” (the English sentence) and “decoder_inputs” (the Spanish sentence), and target is the Spanish sentence offset by one step ahead:"
      ],
      "metadata": {
        "id": "4qa37CddJfAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return ({\n",
        "      \"english\": eng,\n",
        "      # note: the input Spanish sentence doesn't include the last token\n",
        "      #       to keep inputs and targets at the same length\n",
        "      \"spanish\": spa[:, :-1],},\n",
        "          # the target Spanish sentence is one step ahead. Both are still the same length(20 words)\n",
        "          spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "\n",
        "  # use in-memory caching to speed up the preprocessing\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "# create the Datasets for training and validation\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "CZ78dhxzJcho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What our dataset outputs look like:"
      ],
      "metadata": {
        "id": "_XDtCAmyJiSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS_ElPOUJj9u",
        "outputId": "c9f891ac-d750-4f94-9795-3de6494996f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Transformer"
      ],
      "metadata": {
        "id": "MwfVTOdWJlmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "7q1US9C7Jptn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implement the encoder part of the Transformer as a subclassed Layer."
      ],
      "metadata": {
        "id": "ceF1NM7ANj8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(keras.layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # set the size of the input token vectors\n",
        "    self.embed_dim = embed_dim\n",
        "    # set the size of the inner dense layer\n",
        "    self.dense_dim = dense_dim\n",
        "    # set the number of attention heads\n",
        "    self.num_heads = num_heads\n",
        "    # create the multi-head self-attention layer\n",
        "    self.attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    # create the Dense layer with ReLU action\n",
        "    self.dense_proj = keras.Sequential([keras.layers.Dense(dense_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),])\n",
        "    # create normalization layers\n",
        "    self.layernorm_1 = keras.layers.LayerNormalization()\n",
        "    self.layernorm_2 = keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    # expand the rank of the mask generated by the Embedding layer\n",
        "    if mask is not None:\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  # for serialization; so that we can save the model\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "    return config"
      ],
      "metadata": {
        "id": "vwmdv5eZJro3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "dR32dUKDKRbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(keras.layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.attention_2 = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential([keras.layers.Dense(dense_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),])\n",
        "    self.layernorm_1 = keras.layers.LayerNormalization()\n",
        "    self.layernorm_2 = keras.layers.LayerNormalization()\n",
        "    self.layernorm_3 = keras.layers.LayerNormalization()\n",
        "    # ensure that the layer will propagate its input mask to its outputs\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "\n",
        "    return config\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    # generate matrix of shape (sequence_length, sequence_length) with 1s in one half and 0s in the other\n",
        "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "    # replicate it along the batch axis to get a matrix of shape (batch_size, sequence_length, sequence_length)\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat( [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "  def call(self, inputs, encoder_outputs, mask=None):\n",
        "    # get the causal mask\n",
        "    causal_mask = self.get_causal_attention_mask(inputs)\n",
        "    # prepare the input mask which describes padding locations in the target sequence\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "      # merge the two masks together\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "    else:\n",
        "      padding_mask = mask\n",
        "\n",
        "    # pass causal mask to the 1st attention layer which applies self-attention over the target sequence\n",
        "    attention_output_1 = self.attention_1( query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
        "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "    # pass the combined mask to the second attention layer which relates the source sequence to the target sequence\n",
        "    attention_output_2 = self.attention_2(query=attention_output_1, value=encoder_outputs,key=encoder_outputs, attention_mask=padding_mask,)\n",
        "    attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "    proj_output = self.dense_proj(attention_output_2)\n",
        "\n",
        "    return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "awjpCsUaKS3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Positional Embedding layer"
      ],
      "metadata": {
        "id": "rZHF9C7TLdA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implement positional embedding as a subclassed layer."
      ],
      "metadata": {
        "id": "Rcf-Ys8vOo0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(keras.layers.Layer):\n",
        "  # note: we have to know the sequence_length in advance\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    # Embedding layer for the token indices\n",
        "    self.token_embeddings = keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "    # Embedding layer for the token positions\n",
        "    self.position_embeddings = keras.layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    # add both embedding vectors together\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  # like the Embedding layer, this layer should be able to generate a mask\n",
        "  # so we can ignore padding 0s in the inputs.\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  # for serialization; so that we can save the model\n",
        "  def get_config(self):\n",
        "    config = super(PositionalEmbedding, self).get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim,\n",
        "        })\n",
        "    return config"
      ],
      "metadata": {
        "id": "-oE48NZXLgly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting all together"
      ],
      "metadata": {
        "id": "OrSjPVsXMAId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(embed_dim=256, dense_dim=2048, num_heads=8):\n",
        "  encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "  x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "  # encode the source sentence\n",
        "  encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "  decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "  x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "  # encode the target sequence and combine ith with the encoded source sentence\n",
        "  x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "  x = keras.layers.Dropout(0.5)(x)\n",
        "  # predict a word for each output position\n",
        "  decoder_outputs = keras.layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "  return keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "6DwsUez-MBoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = build_transformer()"
      ],
      "metadata": {
        "id": "HgSLh_NwMkDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile and Train the Transformer"
      ],
      "metadata": {
        "id": "J2sO7FiiMuD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(optimizer=\"rmsprop\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kEoLwPQMxbG",
        "outputId": "8a6aa90b-a734-4df5-ad4f-41dd2094e47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1302/1302 [==============================] - 120s 82ms/step - loss: 3.8022 - accuracy: 0.4390 - val_loss: 2.8984 - val_accuracy: 0.5349\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.8523 - accuracy: 0.5493 - val_loss: 2.5109 - val_accuracy: 0.5928\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.5562 - accuracy: 0.5929 - val_loss: 2.3723 - val_accuracy: 0.6157\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.3944 - accuracy: 0.6198 - val_loss: 2.3553 - val_accuracy: 0.6252\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.2892 - accuracy: 0.6379 - val_loss: 2.3146 - val_accuracy: 0.6335\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 2.2155 - accuracy: 0.6518 - val_loss: 2.3056 - val_accuracy: 0.6360\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.1545 - accuracy: 0.6640 - val_loss: 2.2796 - val_accuracy: 0.6434\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.0954 - accuracy: 0.6755 - val_loss: 2.2640 - val_accuracy: 0.6477\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.0431 - accuracy: 0.6854 - val_loss: 2.2330 - val_accuracy: 0.6578\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.9986 - accuracy: 0.6938 - val_loss: 2.2538 - val_accuracy: 0.6595\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.9558 - accuracy: 0.7011 - val_loss: 2.2469 - val_accuracy: 0.6631\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.9269 - accuracy: 0.7061 - val_loss: 2.2511 - val_accuracy: 0.6656\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.8985 - accuracy: 0.7123 - val_loss: 2.2516 - val_accuracy: 0.6651\n",
            "Epoch 14/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.8730 - accuracy: 0.7166 - val_loss: 2.2603 - val_accuracy: 0.6646\n",
            "Epoch 15/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.8519 - accuracy: 0.7210 - val_loss: 2.3149 - val_accuracy: 0.6618\n",
            "Epoch 16/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.8320 - accuracy: 0.7243 - val_loss: 2.3043 - val_accuracy: 0.6674\n",
            "Epoch 17/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 1.8135 - accuracy: 0.7277 - val_loss: 2.2986 - val_accuracy: 0.6670\n",
            "Epoch 18/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7977 - accuracy: 0.7309 - val_loss: 2.3192 - val_accuracy: 0.6674\n",
            "Epoch 19/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.7772 - accuracy: 0.7342 - val_loss: 2.3173 - val_accuracy: 0.6679\n",
            "Epoch 20/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.7611 - accuracy: 0.7373 - val_loss: 2.3278 - val_accuracy: 0.6687\n",
            "Epoch 21/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7457 - accuracy: 0.7398 - val_loss: 2.3491 - val_accuracy: 0.6679\n",
            "Epoch 22/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7293 - accuracy: 0.7423 - val_loss: 2.3659 - val_accuracy: 0.6693\n",
            "Epoch 23/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7175 - accuracy: 0.7450 - val_loss: 2.3487 - val_accuracy: 0.6718\n",
            "Epoch 24/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.7016 - accuracy: 0.7479 - val_loss: 2.3681 - val_accuracy: 0.6678\n",
            "Epoch 25/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6876 - accuracy: 0.7505 - val_loss: 2.4006 - val_accuracy: 0.6682\n",
            "Epoch 26/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6748 - accuracy: 0.7525 - val_loss: 2.4166 - val_accuracy: 0.6698\n",
            "Epoch 27/30\n",
            "1302/1302 [==============================] - 87s 67ms/step - loss: 1.6644 - accuracy: 0.7545 - val_loss: 2.3735 - val_accuracy: 0.6741\n",
            "Epoch 28/30\n",
            "1302/1302 [==============================] - 86s 66ms/step - loss: 1.6518 - accuracy: 0.7569 - val_loss: 2.3951 - val_accuracy: 0.6711\n",
            "Epoch 29/30\n",
            "1302/1302 [==============================] - 88s 67ms/step - loss: 1.6373 - accuracy: 0.7590 - val_loss: 2.4237 - val_accuracy: 0.6717\n",
            "Epoch 30/30\n",
            "1302/1302 [==============================] - 88s 68ms/step - loss: 1.6277 - accuracy: 0.7611 - val_loss: 2.4552 - val_accuracy: 0.6662\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b4a0044e620>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing our Transformer"
      ],
      "metadata": {
        "id": "TMBE9uztM5gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "    predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "Vn9FxtefND9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612ebe02-46d2-48e1-8d99-e72ed768ac7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "What you don't want to do is make Tom angry.\n",
            "[start] qué no quieres hacer enfadar a tom [end]\n",
            "-\n",
            "God created the world in six days.\n",
            "[start] dios no he [UNK] en seis días [end]\n",
            "-\n",
            "Rosa Parks refused to give up her seat for a white passenger.\n",
            "[start] [UNK] cada [UNK] que le dio un libro a un cara blanco en blanco [end]\n",
            "-\n",
            "He proposed that we should play baseball.\n",
            "[start] Él mató a jugar al al béisbol [end]\n",
            "-\n",
            "Why did you learn German?\n",
            "[start] por qué [UNK] alemán [end]\n",
            "-\n",
            "It's not too early.\n",
            "[start] no es demasiado pronto [end]\n",
            "-\n",
            "He is our teacher and a person we should respect.\n",
            "[start] Él es nuestro profesor y deberíamos [UNK] a los desayuno [end]\n",
            "-\n",
            "He has two boys and a girl.\n",
            "[start] tiene dos hijos ni una chica [end]\n",
            "-\n",
            "Keep them.\n",
            "[start] [UNK] [end]\n",
            "-\n",
            "The radio was plugged in.\n",
            "[start] la radio estaba [UNK] [end]\n",
            "-\n",
            "You learn something new every day.\n",
            "[start] tú [UNK] algo nuevo coche [end]\n",
            "-\n",
            "Tom called Mary up yesterday.\n",
            "[start] tom llamó a mary ayer [end]\n",
            "-\n",
            "To drive a car, you need a license.\n",
            "[start] para conducir un coche que necesitas el corazón [end]\n",
            "-\n",
            "When he comes, I'll pay the money that I promised.\n",
            "[start] cuando viene [UNK] el dinero [end]\n",
            "-\n",
            "She stared at him with hatred.\n",
            "[start] ella lo miró con odio [end]\n",
            "-\n",
            "Could I borrow a hammer?\n",
            "[start] podría tomar dinero en un traje [end]\n",
            "-\n",
            "We had hardly started when it began to rain.\n",
            "[start] apenas había empezar cuando empezó a llover [end]\n",
            "-\n",
            "Tom was too shy to take part in games with the other boys.\n",
            "[start] tom fue demasiado tímido para [UNK] en los demás con los otros niños [end]\n",
            "-\n",
            "The dog is white.\n",
            "[start] el perro es blanco [end]\n",
            "-\n",
            "Beer bottles are made of glass.\n",
            "[start] se a comprar cerveza le había [UNK] [end]\n"
          ]
        }
      ]
    }
  ]
}