{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CelikAbdullah/deep-learning-notebooks/blob/main/Natural%20Language%20Processing%20(NLP)/Machine%20Translation/Machine%20Translation%20with%20RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "vQ6VJmSGIuZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5YHOZst_1c-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to implement a sequence-to-sequence modeling on a machine translation task."
      ],
      "metadata": {
        "id": "aS5acFrZzjms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset"
      ],
      "metadata": {
        "id": "8XK9dH4oy4jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have to download an English-to-Spanish translation dataset from the following download link:"
      ],
      "metadata": {
        "id": "-9vgx2fgzyLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWvNyfCKuuL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe627b9-6cbe-4c6e-9786-080f63de4165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-13 20:03:29--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.193.207, 173.194.194.207, 173.194.195.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.193.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-09-13 20:03:29 (206 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete the download, we unzip the .zip file:"
      ],
      "metadata": {
        "id": "vs3h9qra0CbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "id": "W9PIjxqg0JIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse the text file"
      ],
      "metadata": {
        "id": "q2REp7R6zdy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text file contains one example per line:\n",
        "\n",
        "**[an English sentence] [tab character] [corresponding Spanish sentence]**\n",
        "\n",
        "Let's parse the .txt file:"
      ],
      "metadata": {
        "id": "46x-N8zK0LJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "\n",
        "# iterate over the lines in the file\n",
        "for line in lines:\n",
        "  # each line contains an English phrase and its Spanish translation\n",
        "  # a Tab separates them\n",
        "  english, spanish = line.split(\"\\t\")\n",
        "  # prepend [start] and append [end] to the Spanish sentence\n",
        "  spanish = \"[start] \" + spanish + \" [end]\"\n",
        "  text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "hbAw7tJUzgK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print a random sentence to see how it looks like:"
      ],
      "metadata": {
        "id": "HxooIiBt1iZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_example = random.choice(text_pairs)\n",
        "print(random_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS9GcNtr1plJ",
        "outputId": "ff27125e-5b11-496b-9caa-c2bbe4200c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"I'm sorry I was rude to you.\", '[start] Lo siento si fui grosero contigo. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the dataset"
      ],
      "metadata": {
        "id": "0if2P8Cy1xXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's shuffle the dataset and split it into a training, validation and test sets:"
      ],
      "metadata": {
        "id": "vgzyHFB310bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle\n",
        "random.shuffle(text_pairs)\n",
        "# calculate number of validation samples\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "# calculate number of training samples\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "# training set\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "# validation set\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "# test set\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "PhGNy_G41z0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize the English and Spanish text pairs"
      ],
      "metadata": {
        "id": "AxMffrwu2_Fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create two TextVectorization layers:\n",
        "one for English and one for Spanish.\n",
        "\n",
        "For that, we preserve the [start] and [end] tokens that we inserted previously.\n",
        "Keep in mind that punctuation is different in each language. In the Spanish TextVectorization layer, if we are going to strip punctuation characters, we need to also strip the character \"¿\". Normally, we wouldn't do that but for the sake of simplicity, we'll do it here."
      ],
      "metadata": {
        "id": "5lb9gEtj3OMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "# a custom string standardization function\n",
        "# for the Spanish TextVectorization layer:\n",
        "# it preserves [ and ] but strips ¿ as well\n",
        "# as other characters from strings.punctuation\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")"
      ],
      "metadata": {
        "id": "wWd4GwHt34x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to keep things simple, we'll only look at the\n",
        "# top 15000 words in each language\n",
        "vocab_size = 15000\n",
        "# we'll also restrict sentences to 20 words\n",
        "sequence_length = 20\n",
        "\n",
        "# define the English TextVectorization layer\n",
        "source_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "# define the Spanish TextVectorization layer\n",
        "target_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    # recall: that each Spanish sentence starts with the \"[start]\" token\n",
        "    #         so, we need to offset the sentence by one step during training\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "# invoke adapt to learn the vocabulary of each language\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "1N3EwUp73JMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turn data into a tf.data pipeline"
      ],
      "metadata": {
        "id": "8hyYDcPn5ZJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to return a tuple (inputs, target) where inputs is a dict with two key, “encoder_inputs” (the English sentence) and “decoder_inputs” (the Spanish sentence), and target is the Spanish sentence offset by one step ahead:\n"
      ],
      "metadata": {
        "id": "4Dy45v5p5eEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return ({\n",
        "      \"english\": eng,\n",
        "      # note: the input Spanish sentence doesn't include the last token\n",
        "      #       to keep inputs and targets at the same length\n",
        "      \"spanish\": spa[:, :-1],},\n",
        "          # the target Spanish sentence is one step ahead. Both are still the same length(20 words)\n",
        "          spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "\n",
        "  # use in-memory caching to speed up the preprocessing\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "# create the Datasets for training and validation\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "OLYmnTVX52A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What our dataset outputs look like:"
      ],
      "metadata": {
        "id": "usTgHVSA60Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNWgda7u63ny",
        "outputId": "3c1d382c-e80e-49c6-8faa-1d68e85c0081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the RNN"
      ],
      "metadata": {
        "id": "DNPItgiJ_U_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an GRU-based encoder"
      ],
      "metadata": {
        "id": "xi1JrJR8_ldN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_gru(latent_dim, embed_dim):\n",
        "  source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "  x = keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "  encoder_gru = keras.layers.GRU(latent_dim)\n",
        "  encoded_source = keras.layers.Bidirectional(encoder_gru, merge_mode=\"sum\")(x)\n",
        "\n",
        "  return source, encoded_source"
      ],
      "metadata": {
        "id": "JMzbPhzyAxul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an GRU-based decoder"
      ],
      "metadata": {
        "id": "jRHuCpl7BKg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_gru(latent_dim):\n",
        "  return keras.layers.GRU(latent_dim, return_sequences=True)"
      ],
      "metadata": {
        "id": "-vNsMr-TBOTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Seq-to-Seq RNN model"
      ],
      "metadata": {
        "id": "oAA_GXo5Br-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq2seq_rnn(embed_dim = 256, latent_dim = 1024):\n",
        "  past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "  x = keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "  source, encoded_source = encoder_gru(latent_dim, embed_dim)\n",
        "  decoder = decoder_gru(latent_dim)\n",
        "  x = decoder(x, initial_state=encoded_source)\n",
        "  x = keras.layers.Dropout(0.5)(x)\n",
        "  target_next_step = keras.layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "  return keras.Model(inputs=[source, past_target], outputs=target_next_step, name=\"RNN\")\n"
      ],
      "metadata": {
        "id": "EBDVe1GMB0fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "seq2seq_rnn = seq2seq_rnn()"
      ],
      "metadata": {
        "id": "AOARIedJEKYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and train the RNN"
      ],
      "metadata": {
        "id": "Bpol-ZbTEStm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile\n",
        "seq2seq_rnn.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "# train\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "YzZMMYpsEWdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d21cfb3a-1835-4030-96f1-43553fcfdea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1302/1302 [==============================] - 150s 100ms/step - loss: 4.6916 - accuracy: 0.3174 - val_loss: 3.9220 - val_accuracy: 0.3831\n",
            "Epoch 2/15\n",
            "1302/1302 [==============================] - 109s 83ms/step - loss: 3.7318 - accuracy: 0.4156 - val_loss: 3.2394 - val_accuracy: 0.4683\n",
            "Epoch 3/15\n",
            "1302/1302 [==============================] - 109s 84ms/step - loss: 3.2154 - accuracy: 0.4734 - val_loss: 2.8688 - val_accuracy: 0.5154\n",
            "Epoch 4/15\n",
            "1302/1302 [==============================] - 109s 84ms/step - loss: 2.8610 - accuracy: 0.5133 - val_loss: 2.6160 - val_accuracy: 0.5516\n",
            "Epoch 5/15\n",
            "1302/1302 [==============================] - 109s 84ms/step - loss: 2.5875 - accuracy: 0.5464 - val_loss: 2.4408 - val_accuracy: 0.5781\n",
            "Epoch 6/15\n",
            "1302/1302 [==============================] - 109s 84ms/step - loss: 2.3683 - accuracy: 0.5737 - val_loss: 2.3133 - val_accuracy: 0.5974\n",
            "Epoch 7/15\n",
            "1302/1302 [==============================] - 109s 84ms/step - loss: 2.1851 - accuracy: 0.5977 - val_loss: 2.2180 - val_accuracy: 0.6110\n",
            "Epoch 8/15\n",
            "1302/1302 [==============================] - 109s 84ms/step - loss: 2.0296 - accuracy: 0.6179 - val_loss: 2.1441 - val_accuracy: 0.6229\n",
            "Epoch 9/15\n",
            "1302/1302 [==============================] - 109s 84ms/step - loss: 1.8930 - accuracy: 0.6364 - val_loss: 2.0879 - val_accuracy: 0.6318\n",
            "Epoch 10/15\n",
            "1302/1302 [==============================] - 119s 92ms/step - loss: 1.7679 - accuracy: 0.6539 - val_loss: 2.0484 - val_accuracy: 0.6382\n",
            "Epoch 11/15\n",
            "1302/1302 [==============================] - 109s 84ms/step - loss: 1.6648 - accuracy: 0.6684 - val_loss: 2.0121 - val_accuracy: 0.6442\n",
            "Epoch 12/15\n",
            "1302/1302 [==============================] - 108s 83ms/step - loss: 1.5753 - accuracy: 0.6807 - val_loss: 1.9856 - val_accuracy: 0.6483\n",
            "Epoch 13/15\n",
            "1302/1302 [==============================] - 108s 83ms/step - loss: 1.4963 - accuracy: 0.6921 - val_loss: 1.9641 - val_accuracy: 0.6519\n",
            "Epoch 14/15\n",
            "1302/1302 [==============================] - 108s 83ms/step - loss: 1.4226 - accuracy: 0.7030 - val_loss: 1.9460 - val_accuracy: 0.6553\n",
            "Epoch 15/15\n",
            "1302/1302 [==============================] - 108s 83ms/step - loss: 1.3622 - accuracy: 0.7119 - val_loss: 1.9373 - val_accuracy: 0.6574\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a511146a140>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing our Seq2Seq RNN"
      ],
      "metadata": {
        "id": "5-XEo-BTGISN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare a dict to convert token index predictions to string tokens\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  # seed token\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "    # sample the next token\n",
        "    next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "    # convert the next token prediction to a string and append it to the generated sentence\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    # exit condition: either hit max length or a stop character\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "_OyHluGyGXXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a25d07ab-905a-4986-cc0a-f6cf91df7059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Try to fulfill your duty.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[start] intenta [UNK] tu nombre [end]\n",
            "-\n",
            "He is not stupid.\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "[start] Él no es estúpido [end]\n",
            "-\n",
            "He likes to travel by himself.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] le gusta viajar a tiempo [end]\n",
            "-\n",
            "It was unpardonable.\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[start] fue [UNK] [end]\n",
            "-\n",
            "I don't care for the way he talks.\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[start] no me importa por qué está hablando [end]\n",
            "-\n",
            "What is your blood type?\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[start] cuál es tu grupo sanguíneo [end]\n",
            "-\n",
            "He doesn't want to live in the city.\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[start] Él no quiere vivir en el campo [end]\n",
            "-\n",
            "I'm broke.\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[start] soy [UNK] [end]\n",
            "-\n",
            "I want to see more.\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] quiero verlo más [end]\n",
            "-\n",
            "I am baffled.\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[start] estoy [UNK] [end]\n",
            "-\n",
            "What have you learned about Tom so far?\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[start] qué has hecho de tom acerca de lo que estaba aquí [end]\n",
            "-\n",
            "Grab the bottom.\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] [UNK] el suelo [end]\n",
            "-\n",
            "Tom is hitting Mary.\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[start] tom está [UNK] a mary [end]\n",
            "-\n",
            "It's not too late.\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[start] no es demasiado tarde [end]\n",
            "-\n",
            "I've seen that movie many times, but I'd like to see it again.\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[start] he visto a esa película desde que he visto a ver ver antes de ver [end]\n",
            "-\n",
            "Tom and Mary know each other.\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[start] tom y mary se conocen [end]\n",
            "-\n",
            "What's your theory?\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[start] cuál es tu destino [end]\n",
            "-\n",
            "I'm not going to ask Tom for anything.\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[start] no le voy a dar a tom por nada [end]\n",
            "-\n",
            "I can't say anything about that.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[start] no puedo decir nada sobre eso [end]\n",
            "-\n",
            "I'll call you a cab.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[start] te llamaré a un taxi [end]\n"
          ]
        }
      ]
    }
  ]
}